{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from builtins import range, input\n",
    "\n",
    "import os,sys\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM,GRU,Dense,Embedding, Bidirectional, RepeatVector,Concatenate,Activation,Dot,Lambda\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras.backend as K\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/Attention Model Whole Graph.PNG.png\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Softmax over time\n",
    "\n",
    "- make sure we do softmax over the time axis\n",
    "- expected shape is N x T x D\n",
    "- note: the latest version of Keras allows you to pass in axis arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_over_time(x):\n",
    "    assert(K.ndim(x) > 2)\n",
    "    e = K.exp(x - K.max(x,axis = 1,keepdims = True))\n",
    "    s = K.sum(e,axis = 1,keepdims = True)\n",
    "    \n",
    "    return e/s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LATENT_DIM = 256\n",
    "LATENT_DIM_DECODER = 256 # idea: make it different to ensure things all fit together properly!\n",
    "NUM_SAMPLES = 10000\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "MAX_NUM_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "target_texts_inputs = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num samples: 10000\n"
     ]
    }
   ],
   "source": [
    "t = 0\n",
    "for line in open('data/translation/spa.txt',encoding = 'utf8'):\n",
    "    # only keep a limited number of samples\n",
    "    t = t+1\n",
    "    if t > NUM_SAMPLES:\n",
    "        break\n",
    "        \n",
    "    # input and target are separated by tab    \n",
    "    if '\\t' not in line:\n",
    "        continue\n",
    "        \n",
    "    # input and target are separated by tab    \n",
    "    input_text, translation = line.rstrip().split('\\t')\n",
    "    \n",
    "    \n",
    "  # make the target input and output\n",
    "  # recall we'll be using teacher forcing\n",
    "    \n",
    "    target_text = translation + ' <eos>'\n",
    "    target_text_input = '<sos> ' +translation\n",
    "    \n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    target_texts_inputs.append(target_text_input)\n",
    "    \n",
    "print(\"num samples:\",len(input_texts))\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprcoessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2367 unique input tokens.\n",
      "Found 6325 unique output tokens.\n",
      "encoder_data.shape: (10000, 5)\n",
      "encoder_data[0]: [ 0  0  0  0 15]\n",
      "decoder_data[0]: [   2 1491    0    0    0    0    0    0    0]\n",
      "decoder_data.shape: (10000, 9)\n"
     ]
    }
   ],
   "source": [
    "tokenizer_inputs = Tokenizer(num_words = MAX_NUM_WORDS)\n",
    "tokenizer_inputs.fit_on_texts(input_texts)\n",
    "input_sequences = tokenizer_inputs.texts_to_sequences(input_texts)\n",
    "\n",
    "# get the word to index mapping for input language\n",
    "\n",
    "word2idx_inputs = tokenizer_inputs.word_index\n",
    "print('Found %s unique input tokens.'%len(word2idx_inputs))\n",
    "\n",
    "#determine maximum length input sequence\n",
    "\n",
    "max_len_input = max(len(s) for s in input_sequences)\n",
    "\n",
    "# tokenize the outputs\n",
    "# don't filter out special characters\n",
    "# otherwise <sos> and <eos> won't appear\n",
    "\n",
    "tokenizer_outputs = Tokenizer(num_words = MAX_NUM_WORDS, filters = '')\n",
    "tokenizer_outputs.fit_on_texts(target_texts + target_texts_inputs)\n",
    "target_sequences = tokenizer_outputs.texts_to_sequences(target_texts)\n",
    "target_sequences_inputs = tokenizer_outputs.texts_to_sequences(target_texts_inputs)\n",
    "\n",
    "# get the word to index mapping for output language\n",
    "word2idx_outputs = tokenizer_outputs.word_index\n",
    "print('Found %s unique output tokens.' % len(word2idx_outputs))\n",
    "\n",
    "# store number of output words for later\n",
    "# remember to add 1 since indexing starts at 1\n",
    "\n",
    "num_words_output = len(word2idx_outputs) +1\n",
    "\n",
    "\n",
    "# determine maximum length output sequence\n",
    "\n",
    "max_len_target = max(len(s) for s in target_sequences)\n",
    "\n",
    "# pad the sequences\n",
    "\n",
    "encoder_inputs = pad_sequences(input_sequences,maxlen = max_len_input)\n",
    "print('encoder_data.shape:', encoder_inputs.shape)\n",
    "print(\"encoder_data[0]:\", encoder_inputs[0])\n",
    "\n",
    "decoder_inputs = pad_sequences(target_sequences_inputs, maxlen = max_len_target,padding = 'post')\n",
    "print('decoder_data[0]:', decoder_inputs[0])\n",
    "print('decoder_data.shape:', decoder_inputs.shape)\n",
    "\n",
    "decoder_targets = pad_sequences(target_sequences, maxlen = max_len_target,padding = 'post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Embedding Preprcoessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# store all the pre-trained word vectors\n",
    "\n",
    "word2vec = {}\n",
    "with open(os.path.join('large_files/glove.6B/glove.6B.%sd.txt' % EMBEDDING_DIM),encoding = 'utf8') as f:\n",
    "   # is just a space-separated text file in the format:\n",
    "  # word vec[0] vec[1] vec[2] ...\n",
    "    \n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:],dtype = 'float32')\n",
    "        word2vec[word] = vec\n",
    "        \n",
    "print('Found %s word vectors.' % len(word2vec))\n",
    "\n",
    "\n",
    "# prepare embedding matrix\n",
    "\n",
    "\n",
    "num_words = min(MAX_NUM_WORDS,len(word2idx_inputs)+1)\n",
    "embedding_matrix = np.zeros((num_words,EMBEDDING_DIM))\n",
    "for word, i in word2idx_inputs.items():\n",
    "    if i < MAX_NUM_WORDS:\n",
    "        embedding_vector = word2vec.get(word)\n",
    "        \n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "\n",
    "# create embedding layer\n",
    "\n",
    "embedding_layer = Embedding(num_words, EMBEDDING_DIM,\n",
    "                           weights = [embedding_matrix],\n",
    "                           input_length = max_len_input,\n",
    "                           )\n",
    "\n",
    "# create targets, since we cannot use sparse categorical cross entropy when we have sequences\n",
    "\n",
    "decoder_targets_one_hot = np.zeros((len(input_texts),\n",
    "                                 max_len_target,\n",
    "                                 num_words_output), dtype=  'float32')\n",
    "\n",
    "\n",
    "# assign the values\n",
    "\n",
    "for i, d in enumerate(decoder_targets):\n",
    "    for t, word in enumerate(d):\n",
    "        decoder_targets_one_hot[i,t,word] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### build the model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1242: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# Set up the encoder - simple!\n",
    "\n",
    "encoder_inputs_placeholder = Input(shape = (max_len_input,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = Bidirectional(LSTM(LATENT_DIM,\n",
    "                       return_sequences = True,))\n",
    "\n",
    "encoder_outputs = encoder(x)\n",
    "\n",
    "# Set up the decoder - not so simple\n",
    "\n",
    "decoder_inputs_placeholder = Input(shape = (max_len_target,))\n",
    "\n",
    "# this word embedding will not use pre-trained vectors\n",
    "# although you could\n",
    "\n",
    "decoder_embedding = Embedding(num_words_output,EMBEDDING_DIM)\n",
    "decoder_inputs_x = decoder_embedding(decoder_inputs_placeholder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention layer\n",
    "\n",
    "![Attention%20weight-1%20.PNG](attachment:Attention%20weight-1%20.PNG)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Attention layers need to be global because they will be repeated Ty times at the decoder\n",
    "\n",
    "attn_repeat_layer = RepeatVector(max_len_input)\n",
    "attn_concat_layer = Concatenate(axis = -1)\n",
    "attn_dense1 = Dense(10,activation = 'tanh')\n",
    "attn_dense2 = Dense(1,activation = softmax_over_time)\n",
    "attn_dot = Dot(axes = 1) # to perform the weighted sum of alpha[t] * h[t]\n",
    "\n",
    "def one_step_attention(h,st_1):\n",
    "    \n",
    "  # h = h(1), ..., h(Tx), shape = (Tx, LATENT_DIM * 2)\n",
    "  # st_1 = s(t-1), shape = (LATENT_DIM_DECODER,)\n",
    " \n",
    "  # copy s(t-1) Tx times\n",
    "  # now shape = (Tx, LATENT_DIM_DECODER)\n",
    "\n",
    "    st_1 = attn_repeat_layer(st_1)\n",
    "    \n",
    "  # Concatenate all h(t)'s with s(t-1)\n",
    "  # Now of shape (Tx, LATENT_DIM_DECODER + LATENT_DIM * 2)\n",
    "    \n",
    "    x = attn_concat_layer([h,st_1])\n",
    "\n",
    "  # Neural net first layer\n",
    "    x = attn_dense1(x)\n",
    "    \n",
    "  # Neural net second layer with special softmax over time\n",
    "\n",
    "    alphas = attn_dense2(x)\n",
    "    \n",
    "  # \"Dot\" the alphas and the h's\n",
    "  # Remember a.dot(b) = sum over a[t] * b[t]\n",
    "    \n",
    "    context = attn_dot([alphas,h])\n",
    "    \n",
    "    return context\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 5)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 5, 100)       236800      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "s0 (InputLayer)                 (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 5, 512)       731136      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 5, 256)       0           s0[0][0]                         \n",
      "                                                                 lstm_8[0][1]                     \n",
      "                                                                 lstm_8[1][1]                     \n",
      "                                                                 lstm_8[2][1]                     \n",
      "                                                                 lstm_8[3][1]                     \n",
      "                                                                 lstm_8[4][1]                     \n",
      "                                                                 lstm_8[5][1]                     \n",
      "                                                                 lstm_8[6][1]                     \n",
      "                                                                 lstm_8[7][1]                     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 5, 768)       0           bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[21][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[22][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[23][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[24][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[25][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[26][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[27][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[28][0]           \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 repeat_vector_1[29][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 5, 10)        7690        concatenate_1[21][0]             \n",
      "                                                                 concatenate_1[22][0]             \n",
      "                                                                 concatenate_1[23][0]             \n",
      "                                                                 concatenate_1[24][0]             \n",
      "                                                                 concatenate_1[25][0]             \n",
      "                                                                 concatenate_1[26][0]             \n",
      "                                                                 concatenate_1[27][0]             \n",
      "                                                                 concatenate_1[28][0]             \n",
      "                                                                 concatenate_1[29][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 9)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 5, 1)         11          dense_1[21][0]                   \n",
      "                                                                 dense_1[22][0]                   \n",
      "                                                                 dense_1[23][0]                   \n",
      "                                                                 dense_1[24][0]                   \n",
      "                                                                 dense_1[25][0]                   \n",
      "                                                                 dense_1[26][0]                   \n",
      "                                                                 dense_1[27][0]                   \n",
      "                                                                 dense_1[28][0]                   \n",
      "                                                                 dense_1[29][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 9, 100)       632600      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot_1 (Dot)                     (None, 1, 512)       0           dense_2[21][0]                   \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 dense_2[22][0]                   \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 dense_2[23][0]                   \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 dense_2[24][0]                   \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 dense_2[25][0]                   \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 dense_2[26][0]                   \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 dense_2[27][0]                   \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 dense_2[28][0]                   \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "                                                                 dense_2[29][0]                   \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 1, 100)       0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 1, 612)       0           dot_1[21][0]                     \n",
      "                                                                 lambda_24[0][0]                  \n",
      "                                                                 dot_1[22][0]                     \n",
      "                                                                 lambda_25[0][0]                  \n",
      "                                                                 dot_1[23][0]                     \n",
      "                                                                 lambda_26[0][0]                  \n",
      "                                                                 dot_1[24][0]                     \n",
      "                                                                 lambda_27[0][0]                  \n",
      "                                                                 dot_1[25][0]                     \n",
      "                                                                 lambda_28[0][0]                  \n",
      "                                                                 dot_1[26][0]                     \n",
      "                                                                 lambda_29[0][0]                  \n",
      "                                                                 dot_1[27][0]                     \n",
      "                                                                 lambda_30[0][0]                  \n",
      "                                                                 dot_1[28][0]                     \n",
      "                                                                 lambda_31[0][0]                  \n",
      "                                                                 dot_1[29][0]                     \n",
      "                                                                 lambda_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "c0 (InputLayer)                 (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_8 (LSTM)                   [(None, 256), (None, 889856      concatenate_7[0][0]              \n",
      "                                                                 s0[0][0]                         \n",
      "                                                                 c0[0][0]                         \n",
      "                                                                 concatenate_7[1][0]              \n",
      "                                                                 lstm_8[0][1]                     \n",
      "                                                                 lstm_8[0][2]                     \n",
      "                                                                 concatenate_7[2][0]              \n",
      "                                                                 lstm_8[1][1]                     \n",
      "                                                                 lstm_8[1][2]                     \n",
      "                                                                 concatenate_7[3][0]              \n",
      "                                                                 lstm_8[2][1]                     \n",
      "                                                                 lstm_8[2][2]                     \n",
      "                                                                 concatenate_7[4][0]              \n",
      "                                                                 lstm_8[3][1]                     \n",
      "                                                                 lstm_8[3][2]                     \n",
      "                                                                 concatenate_7[5][0]              \n",
      "                                                                 lstm_8[4][1]                     \n",
      "                                                                 lstm_8[4][2]                     \n",
      "                                                                 concatenate_7[6][0]              \n",
      "                                                                 lstm_8[5][1]                     \n",
      "                                                                 lstm_8[5][2]                     \n",
      "                                                                 concatenate_7[7][0]              \n",
      "                                                                 lstm_8[6][1]                     \n",
      "                                                                 lstm_8[6][2]                     \n",
      "                                                                 concatenate_7[8][0]              \n",
      "                                                                 lstm_8[7][1]                     \n",
      "                                                                 lstm_8[7][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 1, 100)       0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 1, 100)       0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 1, 100)       0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 1, 100)       0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 1, 100)       0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 1, 100)       0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 1, 100)       0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 1, 100)       0           embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 6326)         1625782     lstm_8[0][0]                     \n",
      "                                                                 lstm_8[1][0]                     \n",
      "                                                                 lstm_8[2][0]                     \n",
      "                                                                 lstm_8[3][0]                     \n",
      "                                                                 lstm_8[4][0]                     \n",
      "                                                                 lstm_8[5][0]                     \n",
      "                                                                 lstm_8[6][0]                     \n",
      "                                                                 lstm_8[7][0]                     \n",
      "                                                                 lstm_8[8][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 9, 6326)      0           dense_9[0][0]                    \n",
      "                                                                 dense_9[1][0]                    \n",
      "                                                                 dense_9[2][0]                    \n",
      "                                                                 dense_9[3][0]                    \n",
      "                                                                 dense_9[4][0]                    \n",
      "                                                                 dense_9[5][0]                    \n",
      "                                                                 dense_9[6][0]                    \n",
      "                                                                 dense_9[7][0]                    \n",
      "                                                                 dense_9[8][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,123,875\n",
      "Trainable params: 4,123,875\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_lstm = LSTM(LATENT_DIM_DECODER,return_state = True)\n",
    "decoder_dense = Dense(num_words_output,activation = 'softmax')\n",
    "\n",
    "initial_s = Input(shape = (LATENT_DIM_DECODER,),name = 's0')\n",
    "initial_c = Input(shape = (LATENT_DIM_DECODER,),name = 'c0')\n",
    "context_last_word_concat_layer = Concatenate(axis = 2)\n",
    "\n",
    "# Unlike previous seq2seq, we cannot get the output all in one step\n",
    "# Instead we need to do Ty steps And in each of those steps, we need to consider all Tx h's\n",
    "\n",
    "# s, c will be re-assigned in each iteration of the loop\n",
    "\n",
    "s = initial_s\n",
    "c = initial_c\n",
    "outputs = []\n",
    "\n",
    "for t in range(max_len_target):   # Ty times\n",
    "  # get the context using attentio\n",
    "\n",
    "    context = one_step_attention(encoder_outputs,s)\n",
    "    \n",
    "    # we need a different layer for each time step\n",
    "\n",
    "    selector = Lambda(lambda x:x[:,t:t+1])\n",
    "    xt = selector(decoder_inputs_x)\n",
    "    \n",
    "    # combine \n",
    "    decoder_lstm_input = context_last_word_concat_layer([context,xt])\n",
    "    \n",
    "    # pass the combined [context, last word] into the LSTM\n",
    "    # along with [s, c]\n",
    "    # get the new [s, c] and output\n",
    "    o,s,c = decoder_lstm(decoder_lstm_input,initial_state = [s,c])\n",
    "    # final dense layer to get next word prediction\n",
    "    decoder_outputs = decoder_dense(o)\n",
    "    outputs.append(decoder_outputs)\n",
    "    \n",
    "    \n",
    "# 'outputs' is now a list of length Ty\n",
    "# each element is of shape (batch size, output vocab size)\n",
    "# therefore if we simply stack all the outputs into 1 tensor\n",
    "# it would be of shape T x N x D\n",
    "# we would like it to be of shape N x T x D    \n",
    "\n",
    "def stack_and_transpose(x):\n",
    "    # x is a list of length T, each element is a batch_size x output_vocab_size tensor\n",
    "    \n",
    "    x = K.stack(x)\n",
    "    x = K.permute_dimensions(x,pattern = (1,0,2)) # is now batch_size x T x output_vocab_size\n",
    "    \n",
    "    return x\n",
    "\n",
    "# make it a layer\n",
    "\n",
    "stacker = Lambda(stack_and_transpose)\n",
    "outputs = stacker(outputs)\n",
    "\n",
    "###  create the model\n",
    "\n",
    "model = Model(inputs = [encoder_inputs_placeholder,\n",
    "                        decoder_inputs_placeholder,\n",
    "                        initial_s, initial_c],\n",
    "             outputs = outputs)\n",
    "\n",
    "\n",
    "    \n",
    "model.summary()   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1344: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 27s 3ms/step - loss: 2.7354 - acc: 0.6328 - val_loss: 2.6839 - val_acc: 0.6564\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 2.0757 - acc: 0.7085 - val_loss: 2.4960 - val_acc: 0.6708\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 1.8475 - acc: 0.7276 - val_loss: 2.3303 - val_acc: 0.6854\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 1.6770 - acc: 0.7472 - val_loss: 2.2005 - val_acc: 0.7036\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 1.5414 - acc: 0.7642 - val_loss: 2.1281 - val_acc: 0.7143\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 1.4230 - acc: 0.7780 - val_loss: 2.0626 - val_acc: 0.7231\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 1.3214 - acc: 0.7899 - val_loss: 2.0315 - val_acc: 0.7294\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 1.2333 - acc: 0.8004 - val_loss: 1.9878 - val_acc: 0.7367\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 1.1565 - acc: 0.8103 - val_loss: 1.9772 - val_acc: 0.7407\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 1.0925 - acc: 0.8197 - val_loss: 1.9962 - val_acc: 0.7433\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 1.0321 - acc: 0.8277 - val_loss: 1.9717 - val_acc: 0.7466\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.9740 - acc: 0.8369 - val_loss: 1.9845 - val_acc: 0.7442\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.9156 - acc: 0.8437 - val_loss: 1.9602 - val_acc: 0.7479\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.8678 - acc: 0.8513 - val_loss: 1.9664 - val_acc: 0.7461\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.8252 - acc: 0.8578 - val_loss: 1.9764 - val_acc: 0.7473\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.7864 - acc: 0.8639 - val_loss: 1.9973 - val_acc: 0.7447\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.7501 - acc: 0.8700 - val_loss: 2.0079 - val_acc: 0.7465\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.7115 - acc: 0.8746 - val_loss: 2.0260 - val_acc: 0.7439\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.6713 - acc: 0.8810 - val_loss: 2.0239 - val_acc: 0.7456\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.6381 - acc: 0.8862 - val_loss: 2.0528 - val_acc: 0.7426\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 16s 2ms/step - loss: 0.6091 - acc: 0.8909 - val_loss: 2.0614 - val_acc: 0.7405\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.5857 - acc: 0.8959 - val_loss: 2.0578 - val_acc: 0.7427\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.5575 - acc: 0.8993 - val_loss: 2.0765 - val_acc: 0.7379\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.5279 - acc: 0.9048 - val_loss: 2.1100 - val_acc: 0.7357\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.5084 - acc: 0.9082 - val_loss: 2.1138 - val_acc: 0.7343\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.4859 - acc: 0.9122 - val_loss: 2.1283 - val_acc: 0.7373\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.4673 - acc: 0.9137 - val_loss: 2.1431 - val_acc: 0.7331\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.4492 - acc: 0.9164 - val_loss: 2.1582 - val_acc: 0.7323\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.4325 - acc: 0.9195 - val_loss: 2.1684 - val_acc: 0.7323\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 15s 2ms/step - loss: 0.4172 - acc: 0.9228 - val_loss: 2.1824 - val_acc: 0.7306\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.4014 - acc: 0.9247 - val_loss: 2.1947 - val_acc: 0.7313\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.3835 - acc: 0.9275 - val_loss: 2.1965 - val_acc: 0.7307\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.3644 - acc: 0.9313 - val_loss: 2.2101 - val_acc: 0.7293\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.3486 - acc: 0.9333 - val_loss: 2.2370 - val_acc: 0.7287\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.3359 - acc: 0.9359 - val_loss: 2.2531 - val_acc: 0.7273\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.3200 - acc: 0.9378 - val_loss: 2.2570 - val_acc: 0.7267\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.3067 - acc: 0.9396 - val_loss: 2.2841 - val_acc: 0.7275\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.2972 - acc: 0.9416 - val_loss: 2.2886 - val_acc: 0.7264\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.2897 - acc: 0.9422 - val_loss: 2.2963 - val_acc: 0.7261\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.2833 - acc: 0.9435 - val_loss: 2.2836 - val_acc: 0.7251\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.2759 - acc: 0.9454 - val_loss: 2.3243 - val_acc: 0.7254\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.2702 - acc: 0.9457 - val_loss: 2.3348 - val_acc: 0.7238\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.2643 - acc: 0.9462 - val_loss: 2.3401 - val_acc: 0.7231\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.2558 - acc: 0.9474 - val_loss: 2.3387 - val_acc: 0.7253\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.2451 - acc: 0.9480 - val_loss: 2.3611 - val_acc: 0.7214\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.2380 - acc: 0.9489 - val_loss: 2.3618 - val_acc: 0.7236\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.2314 - acc: 0.9497 - val_loss: 2.3674 - val_acc: 0.7233\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.2257 - acc: 0.9502 - val_loss: 2.3627 - val_acc: 0.7227\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.2200 - acc: 0.9508 - val_loss: 2.3803 - val_acc: 0.7234\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.2138 - acc: 0.9515 - val_loss: 2.3925 - val_acc: 0.7215\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.2085 - acc: 0.9524 - val_loss: 2.3757 - val_acc: 0.7226\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.2046 - acc: 0.9519 - val_loss: 2.3866 - val_acc: 0.7233\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.2001 - acc: 0.9527 - val_loss: 2.4076 - val_acc: 0.7211\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1970 - acc: 0.9525 - val_loss: 2.4056 - val_acc: 0.7222\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1937 - acc: 0.9532 - val_loss: 2.4074 - val_acc: 0.7230\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1906 - acc: 0.9530 - val_loss: 2.4349 - val_acc: 0.7229\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1859 - acc: 0.9542 - val_loss: 2.4263 - val_acc: 0.7219\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1823 - acc: 0.9545 - val_loss: 2.4299 - val_acc: 0.7228\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1795 - acc: 0.9541 - val_loss: 2.4527 - val_acc: 0.7208\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1771 - acc: 0.9543 - val_loss: 2.4471 - val_acc: 0.7217\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1736 - acc: 0.9550 - val_loss: 2.4807 - val_acc: 0.7221\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1707 - acc: 0.9548 - val_loss: 2.4781 - val_acc: 0.7228\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1678 - acc: 0.9555 - val_loss: 2.4796 - val_acc: 0.7214\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1643 - acc: 0.9551 - val_loss: 2.4890 - val_acc: 0.7207\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1616 - acc: 0.9554 - val_loss: 2.4872 - val_acc: 0.7219\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1595 - acc: 0.9559 - val_loss: 2.4935 - val_acc: 0.7199\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1562 - acc: 0.9561 - val_loss: 2.5131 - val_acc: 0.7211\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1546 - acc: 0.9553 - val_loss: 2.5079 - val_acc: 0.7206\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1523 - acc: 0.9557 - val_loss: 2.5175 - val_acc: 0.7207\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1499 - acc: 0.9562 - val_loss: 2.5174 - val_acc: 0.7196\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1484 - acc: 0.9558 - val_loss: 2.5388 - val_acc: 0.7211\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 20s 2ms/step - loss: 0.1461 - acc: 0.9565 - val_loss: 2.5344 - val_acc: 0.7208\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1444 - acc: 0.9562 - val_loss: 2.5538 - val_acc: 0.7196\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1420 - acc: 0.9566 - val_loss: 2.5675 - val_acc: 0.7199\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1407 - acc: 0.9562 - val_loss: 2.5832 - val_acc: 0.7188\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1389 - acc: 0.9567 - val_loss: 2.5819 - val_acc: 0.7200\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1370 - acc: 0.9567 - val_loss: 2.6014 - val_acc: 0.7181\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1354 - acc: 0.9565 - val_loss: 2.5839 - val_acc: 0.7204\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1337 - acc: 0.9564 - val_loss: 2.6121 - val_acc: 0.7191\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1322 - acc: 0.9565 - val_loss: 2.5874 - val_acc: 0.7197\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1304 - acc: 0.9570 - val_loss: 2.6108 - val_acc: 0.7189\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1291 - acc: 0.9566 - val_loss: 2.6252 - val_acc: 0.7189\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1285 - acc: 0.9568 - val_loss: 2.6264 - val_acc: 0.7194\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 20s 2ms/step - loss: 0.1266 - acc: 0.9565 - val_loss: 2.6170 - val_acc: 0.7187\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 20s 2ms/step - loss: 0.1257 - acc: 0.9564 - val_loss: 2.6351 - val_acc: 0.7203\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1250 - acc: 0.9565 - val_loss: 2.6376 - val_acc: 0.7186\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1234 - acc: 0.9566 - val_loss: 2.6486 - val_acc: 0.7188\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1229 - acc: 0.9564 - val_loss: 2.6504 - val_acc: 0.7205\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1213 - acc: 0.9559 - val_loss: 2.6516 - val_acc: 0.7196\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1205 - acc: 0.9568 - val_loss: 2.6642 - val_acc: 0.7202\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1193 - acc: 0.9557 - val_loss: 2.6609 - val_acc: 0.7194\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1181 - acc: 0.9568 - val_loss: 2.6745 - val_acc: 0.7207\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1170 - acc: 0.9563 - val_loss: 2.6720 - val_acc: 0.7186\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1162 - acc: 0.9565 - val_loss: 2.6772 - val_acc: 0.7193\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1152 - acc: 0.9558 - val_loss: 2.6851 - val_acc: 0.7191\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1148 - acc: 0.9557 - val_loss: 2.7033 - val_acc: 0.7200\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1135 - acc: 0.9569 - val_loss: 2.7093 - val_acc: 0.7198\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1132 - acc: 0.9560 - val_loss: 2.7042 - val_acc: 0.7193\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1122 - acc: 0.9562 - val_loss: 2.7137 - val_acc: 0.7189\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1117 - acc: 0.9554 - val_loss: 2.7238 - val_acc: 0.7183\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer = 'rmsprop',loss = 'categorical_crossentropy',metrics = ['accuracy'])\n",
    "\n",
    "z = np.zeros((NUM_SAMPLES,LATENT_DIM_DECODER)) ##  initial [s, c]\n",
    "r = model.fit([encoder_inputs, decoder_inputs,z,z],decoder_targets_one_hot,\n",
    "             batch_size = BATCH_SIZE,\n",
    "             epochs = EPOCHS,\n",
    "             validation_split = 0.2,\n",
    "             verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot some data\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# accuracies\n",
    "plt.plot(r.history['acc'], label='acc')\n",
    "plt.plot(r.history['val_acc'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# As with the poetry example, we need to create another model\n",
    "# that can take in the RNN state and previous word as input\n",
    "# and accept a T=1 sequence.\n",
    "\n",
    "# The encoder will be stand-alone\n",
    "# From this we will get our initial decoder hidden state\n",
    "# i.e. h(1), ..., h(Tx)\n",
    "encoder_model = Model(encoder_inputs_placeholder, encoder_outputs)\n",
    "\n",
    "# next we define a T=1 decoder model\n",
    "encoder_outputs_as_input = Input(shape=(max_len_input, LATENT_DIM * 2,))\n",
    "decoder_inputs_single = Input(shape=(1,))\n",
    "decoder_inputs_single_x = decoder_embedding(decoder_inputs_single)\n",
    "\n",
    "# no need to loop over attention steps this time because there is only one step\n",
    "context = one_step_attention(encoder_outputs_as_input, initial_s)\n",
    "\n",
    "# combine context with last word\n",
    "decoder_lstm_input = context_last_word_concat_layer([context, decoder_inputs_single_x])\n",
    "\n",
    "# lstm and final dense\n",
    "o, s, c = decoder_lstm(decoder_lstm_input, initial_state=[initial_s, initial_c])\n",
    "decoder_outputs = decoder_dense(o)\n",
    "\n",
    "\n",
    "# note: we don't really need the final stack and tranpose\n",
    "# because there's only 1 output\n",
    "# it is already of size N x D\n",
    "# no need to make it 1 x N x D --> N x 1 x D\n",
    "\n",
    "\n",
    "\n",
    "# create the model object\n",
    "decoder_model = Model(\n",
    "  inputs=[\n",
    "    decoder_inputs_single,\n",
    "    encoder_outputs_as_input,\n",
    "    initial_s, \n",
    "    initial_c\n",
    "  ],\n",
    "  outputs=[decoder_outputs, s, c]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# map indexes back into real words\n",
    "# so we can view the results\n",
    "idx2word_eng = {v:k for k, v in word2idx_inputs.items()}\n",
    "idx2word_trans = {v:k for k, v in word2idx_outputs.items()}\n",
    "\n",
    "\n",
    "def decode_sequence(input_seq):\n",
    "  # Encode the input as state vectors.\n",
    "    enc_out = encoder_model.predict(input_seq)\n",
    "\n",
    "  # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "  \n",
    "  # Populate the first character of target sequence with the start character.\n",
    "  # NOTE: tokenizer lower-cases all words\n",
    "    target_seq[0, 0] = word2idx_outputs['<sos>']\n",
    "\n",
    "  # if we get this we break\n",
    "    eos = word2idx_outputs['<eos>']\n",
    "\n",
    "\n",
    "  # [s, c] will be updated in each loop iteration\n",
    "    s = np.zeros((1, LATENT_DIM_DECODER))\n",
    "    c = np.zeros((1, LATENT_DIM_DECODER))\n",
    "\n",
    "\n",
    "  # Create the translation\n",
    "    output_sentence = []\n",
    "    for _ in range(max_len_target):\n",
    "        o, s, c = decoder_model.predict([target_seq, enc_out, s, c])\n",
    "        \n",
    "\n",
    "    # Get next word\n",
    "        idx = np.argmax(o.flatten())\n",
    "\n",
    "    # End sentence of EOS\n",
    "        if eos == idx:\n",
    "            break\n",
    "\n",
    "      \n",
    "        word = ''\n",
    "        if idx > 0:\n",
    "            word = idx2word_trans[idx]\n",
    "            output_sentence.append(word)\n",
    "\n",
    "    # Update the decoder input\n",
    "    # which is just the word just generated\n",
    "        target_seq[0, 0] = idx\n",
    "\n",
    "    return ' '.join(output_sentence)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Translate sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sentence: Tom likes me.\n",
      "Predicted translation: le gusto a tom.\n",
      "Actual translation: Le gusto a Tom. <eos>\n"
     ]
    }
   ],
   "source": [
    "i = np.random.choice(len(input_texts))\n",
    "input_seq = encoder_inputs[i:i+1]\n",
    "translation = decode_sequence(input_seq)\n",
    "\n",
    "print('Input sentence:', input_texts[i])\n",
    "print('Predicted translation:', translation)\n",
    "print('Actual translation:', target_texts[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
